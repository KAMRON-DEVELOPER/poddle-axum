logging {
	level  = "info"
	format = "logfmt"
}

// ---------------------------------------------------------
// LOGS COLLECTION
// ---------------------------------------------------------

discovery.kubernetes "pods" {
	role = "pod"

	selectors {
		role  = "pod"
		field = "spec.nodeName=" + coalesce(env("NODE_NAME"), constants.hostname)
	}
}

discovery.relabel "pod_logs" {
	targets = discovery.kubernetes.pods.targets

	// Construct the path to the log file on the host
	// Source: namespace/pod_name/uid/container_name
	// Target: /var/log/pods/namespace_pod_name_uid/container_name/*.log
	// Path format: /var/log/pods/<namespace>_<pod_name>_<uid>/<container_name>/*.log
	rule {
		source_labels = [
			"__meta_kubernetes_namespace",
			"__meta_kubernetes_pod_name",
			"__meta_kubernetes_pod_uid",
			"__meta_kubernetes_pod_container_name",
		]
		separator = "/"
		// Capture: (namespace) / (pod_name) / (uid) / (container)
		regex = "([^/]+)/([^/]+)/([^/]+)/([^/]+)"
		// K8s standard path: /var/log/pods/<ns>_<name>_<uid>/<container>/*.log
		replacement  = "/var/log/pods/$1_$2_$3/$4/*.log"
		target_label = "__path__"
	}

	// Assign Job Label
	rule {
		source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_name"]
		separator     = "/"
		target_label  = "job"
	}

	// Standard Labels
	rule {
		source_labels = ["__meta_kubernetes_namespace"]
		target_label  = "namespace"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_name"]
		target_label  = "pod"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_container_name"]
		target_label  = "container"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_label_app"]
		target_label  = "app"
	}

	// Custom Labels
	rule {
		source_labels = ["__meta_kubernetes_pod_label_project_id"]
		target_label  = "project_id"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_label_deployment_id"]
		target_label  = "deployment_id"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_label_managed_by"]
		target_label  = "managed_by"
	}
}

// Expand the wildcards (*.log) into actual file paths
local.file_match "pod_logs" {
	path_targets = discovery.relabel.pod_logs.output
}

// Read the files directly from disk (Efficient!)
loki.source.file "pods" {
	targets    = local.file_match.pod_logs.targets
	forward_to = [loki.process.default.receiver]
}

loki.process "default" {
	forward_to = [loki.write.default.receiver]

	stage.drop {
		older_than          = "1h"
		drop_counter_reason = "too old"
	}

	// Tag everything as containerd (since K3s uses containerd)
	stage.static_labels {
		values = {
			tmp_container_runtime = "containerd",
		}
	}

	stage.json {
		expressions = {
			level    = "level",
			trace_id = "trace_id",
			message  = "message",
			msg      = "msg",
			ts       = "timestamp",
		}
	}

	// If JSON parsing failed (level is empty), fall back to the raw line
	stage.template {
		source   = "msg"
		template = "{{ if .level }}{{ .msg }}{{ else }}{{ .Entry }}{{ end }}"
	}

	// Promotes level from extracted field â†’ Loki label
	// You can filter {level="error"} in Loki, Improves query ergonomics
	stage.labels {
		values = {
			level    = "level",
			trace_id = "trace_id",
		}
	}

	stage.match {
		selector = "{tmp_container_runtime=\"containerd\"}"

		stage.cri { } // The CRI stage is designed to parse logs originating from containerd or CRI-O runtimes

		stage.labels {
			values = {
				flags  = "", // Incorporates 'flags' derived from CRI parsing as a label
				stream = "", // Incorporates 'stream' (stdout/stderr) from CRI parsing as a label
			}
		}
	}

	// Cleanup temporary labels
	stage.label_drop {
		values = ["tmp_container_runtime"]
	}
}

loki.write "default" {
	endpoint {
		url = "http://loki.loki.svc.cluster.local:3100/loki/api/v1/push"
	}
}

// ---------------------------------------------------------
// CONTAINER METRICS (via kubelet's built-in cAdvisor)
// ---------------------------------------------------------

// Discover all nodes in the cluster
discovery.kubernetes "nodes" {
	role = "node"
}

// Relabel to construct kubelet metrics endpoint
discovery.relabel "cadvisor" {
	targets = discovery.kubernetes.nodes.targets

	// Keep only the current node
	rule {
		source_labels = ["__meta_kubernetes_node_name"]
		regex         = env("NODE_NAME")
		action        = "keep"
	}

	// Use the node's internal IP
	rule {
		source_labels = ["__address__"]
		replacement   = "$1:10250"
		target_label  = "__address__"
	}

	// Set the metrics path to kubelet's cAdvisor endpoint
	rule {
		replacement  = "/metrics/cadvisor"
		target_label = "__metrics_path__"
	}

	// Set scheme to https (kubelet uses TLS)
	rule {
		replacement  = "https"
		target_label = "__scheme__"
	}

	// Add node label
	rule {
		source_labels = ["__meta_kubernetes_node_name"]
		target_label  = "node"
	}
}

// Scrape container metrics from kubelet
prometheus.scrape "cadvisor" {
	targets           = discovery.relabel.cadvisor.output
	job_name          = "cadvisor"
	scheme            = "https"
	bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"

	tls_config {
		// Point to the K3s CA via the host mount
		ca_file              = "/host/root/var/lib/rancher/k3s/agent/server-ca.crt"
		server_name          = env("NODE_NAME")
		insecure_skip_verify = false
		// K3s generates self-signed certificates for kubelets
		// Using bearer token auth is sufficient, no client certs needed
	}

	forward_to = [prometheus.remote_write.default.receiver]
}

// ---------------------------------------------------------
// HOST/NODE METRICS
// ---------------------------------------------------------

prometheus.exporter.unix "host" {
	rootfs_path = "/host/root"
	procfs_path = "/host/proc"
	sysfs_path  = "/host/sys"

	// set_collectors = ["cpu", "diskstats", "filesystem", "loadavg", "meminfo", "netdev", "stat", "time", "uname"]
}

prometheus.scrape "node" {
	targets    = prometheus.exporter.unix.host.targets
	job_name   = "node"
	forward_to = [prometheus.remote_write.default.receiver]
}

// ---------------------------------------------------------
// REMOTE WRITE
// ---------------------------------------------------------

prometheus.remote_write "default" {
	endpoint {
		url = "http://prometheus-server.prometheus.svc.cluster.local:80/api/v1/write"
	}
}
