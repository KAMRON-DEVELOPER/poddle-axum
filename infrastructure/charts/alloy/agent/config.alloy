logging {
	level  = "info"
	format = "logfmt"
}

// ---------------------------------------------------------
// LOGS COLLECTION
// ---------------------------------------------------------

discovery.kubernetes "pods" {
	role = "pod"

	selectors {
		role  = "pod"
		field = "spec.nodeName=" + coalesce(env("NODE_NAME"), constants.hostname)
	}
}

discovery.relabel "pod_logs" {
	targets = discovery.kubernetes.pods.targets

	// Construct the path to the log file on the host
	// Source: namespace/pod_name/uid/container_name
	// Target: /var/log/pods/namespace_pod_name_uid/container_name/*.log
	// Path format: /var/log/pods/<namespace>_<pod_name>_<uid>/<container_name>/*.log
	rule {
		source_labels = [
			"__meta_kubernetes_namespace",
			"__meta_kubernetes_pod_name",
			"__meta_kubernetes_pod_uid",
			"__meta_kubernetes_pod_container_name",
		]
		separator = "/"
		// Capture: (namespace) / (pod_name) / (uid) / (container)
		regex = "([^/]+)/([^/]+)/([^/]+)/([^/]+)"
		// K8s standard path: /var/log/pods/<ns>_<name>_<uid>/<container>/*.log
		replacement  = "/var/log/pods/$1_$2_$3/$4/*.log"
		target_label = "__path__"
	}

	// Assign Job Label
	rule {
		source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_name"]
		separator     = "/"
		target_label  = "job"
	}

	// Standard Labels
	rule {
		source_labels = ["__meta_kubernetes_namespace"]
		target_label  = "namespace"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_name"]
		target_label  = "pod"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_container_name"]
		target_label  = "container"
	}

	// Custom Labels
	rule {
		source_labels = ["__meta_kubernetes_pod_label_poddle_io_managed_by"]
		target_label  = "managed_by"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_label_poddle_io_project_id"]
		target_label  = "project_id"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_label_poddle_io_deployment_id"]
		target_label  = "deployment_id"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_label_poddle_io_preset_id"]
		target_label  = "preset_id"
	}
}

// Expand the wildcards (*.log) into actual file paths
local.file_match "pod_logs" {
	path_targets = discovery.relabel.pod_logs.output
}

// Read the files directly from disk (Efficient!)
loki.source.file "pods" {
	targets    = discovery.relabel.pod_logs.output
	forward_to = [loki.process.default.receiver]

	file_match {
		enabled           = true
		ignore_older_than = "0s"
		sync_period       = "10s"
	}
}

loki.process "default" {
	forward_to = [loki.write.default.receiver]

	stage.drop {
		older_than          = "1h"
		drop_counter_reason = "too old"
	}

	// Always parse CRI first
	// <time> <stream> <flags> <log>
	// "2024-01-01T10:00:00.123Z stdout F {"msg": "hello"}"
	stage.cri { }

	// The stage.decolorize strips ANSI color codes from the log lines, making it easier to parse logs.
	// stage.decolorize { }

	stage.tenant {
		label = "preset_id"
	}

	stage.match {
		selector = "{managed_by=\"poddle\"}"

		// Storage impact 0
		// Query shape impact 0
		// Attempt JSON extraction
		stage.json {
			expressions = {
				level     = "level",
				msg       = "msg",
				message   = "message",
				timestamp = "timestamp",
				time      = "time",
				ts        = "ts",
				trace_id  = "trace_id",
				span_id   = "span_id",
				latency   = "latency",
				status    = "status",
			}
		}

		// Attempt Logfmt extraction
		stage.logfmt {
			mapping = {
				level     = "level",
				msg       = "msg",
				message   = "message",
				timestamp = "timestamp",
				time      = "time",
				ts        = "ts",
				trace_id  = "trace_id",
				span_id   = "span_id",
				latency   = "latency",
				status    = "status",
			}
		}

		// without stage.output normalized message useless
		// Normalize message field for frontend
		stage.template {
			source   = "message"
			template = "{{ if .msg }}{{ .msg }}{{ else if .message }}{{ .message }}{{ end }}"
		}

		// Normalize timestamp field for frontend
		stage.template {
			source   = "timestamp"
			template = "{{ if .ts }}{{ .ts }}{{ else if .time }}{{ .time }}{{ else if .timestamp }}{{ .timestamp }}{{ end }}"
		}

		stage.timestamp {
			source            = "timestamp"
			format            = "RFC3339"
			action_on_failure = "skip"
		}
	}

	stage.labels {
		values = {
			level  = "level",
			stream = "",
		}
	}

	stage.structured_metadata {
		values = {
			trace_id = "trace_id",
			span_id  = "span_id",
		}
	}
}

loki.write "default" {
	endpoint {
		url = "http://loki.loki.svc.cluster.local:3100/loki/api/v1/push"
	}
}

// ---------------------------------------------------------
// CONTAINER METRICS (via kubelet's built-in cAdvisor)
// ---------------------------------------------------------

// Discover all nodes in the cluster
discovery.kubernetes "nodes" {
	role = "node"
}

// Relabel to construct kubelet metrics endpoint
discovery.relabel "cadvisor" {
	targets = discovery.kubernetes.nodes.targets

	// Keep only the current node
	rule {
		source_labels = ["__meta_kubernetes_node_name"]
		regex         = env("NODE_NAME")
		action        = "keep"
	}

	// Use the node's internal IP
	rule {
		source_labels = ["__address__"]
		replacement   = "$1:10250"
		target_label  = "__address__"
	}

	// Set the metrics path to kubelet's cAdvisor endpoint
	rule {
		replacement  = "/metrics/cadvisor"
		target_label = "__metrics_path__"
	}

	// Set scheme to https (kubelet uses TLS)
	rule {
		replacement  = "https"
		target_label = "__scheme__"
	}

	// Add node label
	rule {
		source_labels = ["__meta_kubernetes_node_name"]
		target_label  = "node"
	}
}

// Scrape container metrics from kubelet
prometheus.scrape "cadvisor" {
	targets           = discovery.relabel.cadvisor.output
	job_name          = "cadvisor"
	scheme            = "https"
	bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"

	tls_config {
		// Point to the K3s CA via the host mount
		ca_file              = "/host/root/var/lib/rancher/k3s/agent/server-ca.crt"
		server_name          = env("NODE_NAME")
		insecure_skip_verify = false
		// K3s generates self-signed certificates for kubelets
		// Using bearer token auth is sufficient, no client certs needed
	}

	forward_to = [prometheus.remote_write.default.receiver]
}

// ---------------------------------------------------------
// HOST/NODE METRICS
// ---------------------------------------------------------

prometheus.exporter.unix "host" {
	rootfs_path = "/host/root"
	procfs_path = "/host/proc"
	sysfs_path  = "/host/sys"

	// set_collectors = ["cpu", "diskstats", "filesystem", "loadavg", "meminfo", "netdev", "stat", "time", "uname"]
}

prometheus.scrape "node" {
	targets    = prometheus.exporter.unix.host.targets
	job_name   = "node"
	forward_to = [prometheus.remote_write.default.receiver]
}

// ---------------------------------------------------------
// REMOTE WRITE
// ---------------------------------------------------------

prometheus.remote_write "default" {
	endpoint {
		url = "http://prometheus-server.prometheus.svc.cluster.local:80/api/v1/write"
	}
}
