logging {
	level  = "info"
	format = "logfmt"
}

// ---------------------------------------------------------
// LOGS COLLECTION
// ---------------------------------------------------------

discovery.kubernetes "pods" {
	role = "pod"

	selectors {
		role  = "pod"
		field = "spec.nodeName=" + coalesce(env("NODE_NAME"), constants.hostname)
	}
}

discovery.relabel "logs" {
	targets = discovery.kubernetes.pods.targets

	// Drop any targets that do not have the value "poddle" in their "managed_by" label.
	rule {
		source_labels = ["__meta_kubernetes_pod_label_poddle_io_managed_by"]
		regex         = "poddle"
		action        = "keep"
	}

	// Construct the path to the log file on the host
	rule {
		source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
		separator     = "/"
		replacement   = "/var/log/pods/*$1/*.log"
		target_label  = "__path__"
	}

	// Standard Labels
	rule {
		source_labels = ["__meta_kubernetes_namespace"]
		target_label  = "namespace"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_name"]
		target_label  = "pod"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_container_name"]
		target_label  = "container"
	}

	// Custom Labels
	rule {
		source_labels = ["__meta_kubernetes_pod_label_poddle_io_managed_by"]
		target_label  = "managed_by"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_label_poddle_io_project_id"]
		target_label  = "project_id"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_label_poddle_io_deployment_id"]
		target_label  = "deployment_id"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_label_poddle_io_preset_id"]
		target_label  = "preset_id"
	}
}

// Expand the wildcards (*.log) into actual file paths
// local.file_match "logs" {
// 	path_targets = discovery.relabel.logs.output
// }

// Read the files directly from disk (Efficient!)
loki.source.file "pods" {
	// targets    = local.file_match.logs.targets
	targets = discovery.relabel.logs.output

	// Configure file discovery using glob patterns for automatic target discovery.
	// Enables built-in file discovery directly within loki.source.file. When
	// enabled, the component discovers files on the local filesystem using glob patterns and
	// the doublestar library.
	// Can use glob patterns, for example, /tmp/*.log or /var/log/**/*.log, directly
	// in the targets argument’s __path__ label.
	file_match {
		enabled     = true
		sync_period = "10s"
	}

	forward_to = [loki.process.default.receiver]
}

loki.process "default" {
	stage.drop {
		older_than          = "10m"
		drop_counter_reason = "too old"
	}

	// Enables a predefined pipeline which reads log lines using the CRI logging format.
	// CRI specifies log lines as single space-delimited values with the following components:
	//	`time`: The timestamp string of the log.
	//	`stream`: Either stdout or stderr.
	//	`flags`: CRI flags including F or P.
	//	`log`: The contents of the log line.
	// The cri processing stage extracts the following k/v pairs: log, stream, time, flags
	// https://github.com/grafana/k8s-monitoring-helm/blob/main/charts/k8s-monitoring/docs/examples/pod-labels-and-annotations/alloy-logs.alloy
	stage.cri { }

	// Strips ANSI color codes from the log lines, making it easier to parse logs.
	// stage.decolorize { }

	// The source field defines the source of data to parse as logfmt. When source is missing or empty,
	// the stage parses the log line itself, but it can also be used to parse a previously extracted value.
	stage.logfmt {
		source  = "log"
		mapping = {
			level     = "level",
			at        = "at",
			msg       = "msg",
			message   = "message",
			timestamp = "timestamp",
			time      = "time",
			ts        = "ts",
			trace_id  = "trace_id",
			span_id   = "span_id",
			latency   = "latency",
			status    = "status",
			preset_id = "preset_id",
		}
	}

	// Attempt JSON extraction
	stage.json {
		source      = "log"
		expressions = {
			level     = "level",
			at        = "at",
			msg       = "msg",
			message   = "message",
			timestamp = "timestamp",
			time      = "time",
			ts        = "ts",
			trace_id  = "trace_id",
			span_id   = "span_id",
			latency   = "latency",
			status    = "status",
			preset_id = "preset_id",
		}
	}

	// Manipulating and standardizing data from previous stages before setting them as labels in a subsequent stage.
	stage.template {
		source   = "message"
		template = "{{ if .msg }}{{ .msg }}{{ else if .message }}{{ .message }}{{ end }}"
	}

	stage.template {
		source   = "timestamp"
		template = "{{ if .ts }}{{ .ts }}{{ else if .time }}{{ .time }}{{ else if .timestamp }}{{ .timestamp }}{{ end }}"
	}

	// When no timestamp stage is set, the log entry timestamp defaults to the time when the log entry was scraped.
	stage.timestamp {
		source           = "timestamp"
		format           = "RFC3339"
		fallback_formats = ["RFC3339Nano", "Unix", "UnixMs", "UnixUs", "UnixNs"]
		// fudge (default): Change the timestamp to the last known timestamp, summing up 1 nanosecond to guarantee log entries ordering.
		// skip: Don’t change the timestamp and keep the time when the log entry was scraped.
		action_on_failure = "skip"
	}

	// Can read data from the extracted values map and set new labels on incoming log entries
	stage.labels {
		values = {
			preset_id = "preset_id",
			level     = "level", // Sets up a 'level' label, based on the 'level' extracted value.
			stream    = "",      // Sets up an 'stream' label, based on the 'stream' extracted value.
		}
	}

	// Sets the tenant ID for the log entry by obtaining it from a field in the
	// extracted data map, a label, or a provided value.
	stage.tenant {
		label = "preset_id"
	}

	// Can read data from the extracted values map and add them to log entries as structured metadata
	// If you add labels to structured metadata, the labels are removed from the label map.
	stage.structured_metadata {
		values = {
			trace_id = "trace_id",
			span_id  = "span_id",
		}
	}

	// Configures a processing stage that filters the label set of an incoming log entry down to a subset
	stage.label_keep {
		values = ["namespace", "managed_by", "project_id", "deployment_id", "preset_id", "stream", "level"]
	}

	forward_to = [loki.write.default.receiver]
}

loki.write "default" {
	endpoint {
		url                 = "http://loki.loki.svc.cluster.local:3100/loki/api/v1/push"
		batch_size          = "1MiB"
		batch_wait          = "1s"
		max_backoff_period  = "5m"
		max_backoff_retries = 10
		min_backoff_period  = "500ms"
		remote_timeout      = "10s"
	}
}

// ---------------------------------------------------------
// CONTAINER METRICS (via kubelet's built-in cAdvisor)
// ---------------------------------------------------------

// Discover all nodes in the cluster
discovery.kubernetes "nodes" {
	role = "node"
}

// Relabel to construct kubelet metrics endpoint
discovery.relabel "kubelet" {
	targets = discovery.kubernetes.nodes.targets

	// Keep only the current node
	rule {
		source_labels = ["__meta_kubernetes_node_name"]
		regex         = env("NODE_NAME")
		action        = "keep"
	}

	// Use the node's internal IP
	rule {
		source_labels = ["__address__"]
		replacement   = "$1:10250"
		target_label  = "__address__"
	}

	// Set the metrics path to kubelet's cAdvisor endpoint
	rule {
		replacement  = "/metrics/cadvisor"
		target_label = "__metrics_path__"
	}

	// Set scheme to https (kubelet uses TLS)
	rule {
		replacement  = "https"
		target_label = "__scheme__"
	}

	// Add node label
	rule {
		source_labels = ["__meta_kubernetes_node_name"]
		target_label  = "node"
	}
}

// Scrape container metrics from kubelet
prometheus.scrape "kubelet" {
	targets         = discovery.relabel.kubelet.output
	job_name        = "kubelet"
	scrape_interval = "10s"
	scheme          = "https"

	tls_config {
		// Point to the K3s CA via the host mount
		server_name          = env("NODE_NAME")
		ca_file              = "/host/root/var/lib/rancher/k3s/agent/server-ca.crt"
		insecure_skip_verify = false
		// K3s generates self-signed certificates for kubelets
		// Using bearer token auth is sufficient, no client certs needed
	}
	bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"

	forward_to = [prometheus.remote_write.default.receiver]
}

// ---------------------------------------------------------
// HOST/NODE METRICS
// ---------------------------------------------------------

prometheus.exporter.unix "host" {
	rootfs_path    = "/host/root"
	procfs_path    = "/host/proc"
	sysfs_path     = "/host/sys"
	udev_data_path = "/host/root/run/udev/data"

	// set_collectors = ["cpu", "diskstats", "filesystem", "loadavg", "meminfo", "netdev", "stat", "time", "uname"]
}

prometheus.scrape "host" {
	targets         = prometheus.exporter.unix.host.targets
	job_name        = "host"
	scrape_interval = "10s"

	forward_to = [prometheus.remote_write.default.receiver]
}

// ---------------------------------------------------------
// REMOTE WRITE
// ---------------------------------------------------------

prometheus.remote_write "default" {
	endpoint {
		url = "http://prometheus-server.prometheus.svc.cluster.local:80/api/v1/write"
	}
}
